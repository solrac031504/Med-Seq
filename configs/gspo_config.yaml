# ============================================================
#  GSPO Configuration for Qwen/Qwen2-VL-2B
# ============================================================

# ------------------------------
# Model Configuration
# ------------------------------
model_name_or_path: "Qwen/Qwen2-VL-2B"
torch_dtype: "bfloat16"
attn_implementation: "sdpa"
gradient_checkpointing: true
trust_remote_code: true

# ------------------------------
# Training Data Configuration
# ------------------------------
train_data_dir: "/lustre/fs1/home/in642270/GSPO_MED/Splits/modality/train"
image_root: "/lustre/fs1/home/in642270/GSPO_MED/Images"
use_images: true
shuffle: true

# ------------------------------
# Output Configuration
# ------------------------------
output_dir: "/lustre/fs1/home/in642270/GSPO_MED/outputs_gspo_vqa"
save_total_limit: 3
save_strategy: "steps"
save_steps: 200

# ------------------------------
# GSPO Algorithm Parameters
# ------------------------------
num_generations: 2               # group size K for GSPO
gamma: 0.99                      # discount factor
lambda_: 0.95                    # GAE lambda
clip_range: 0.2
entropy_coef: 0.001
vf_coef: 1.0
max_grad_norm: 1.0

# ------------------------------
# Optimization Hyperparameters
# ------------------------------
learning_rate: 1e-5
lr_scheduler_type: "linear"
warmup_steps: 100
weight_decay: 0.01
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
max_steps: 500
log_every_n_steps: 10

# ------------------------------
# Generation Settings
# ------------------------------
max_new_tokens: 64
temperature: 0.7
top_p: 0.9
do_sample: true
repetition_penalty: 1.05

# ------------------------------
# Reward Configuration
# ------------------------------
reward_functions:
  - "accuracy"
  - "format"
reward_clip_value: 1.0

# ------------------------------
# Miscellaneous
# ------------------------------
seed: 42
bf16: true
dataloader_num_workers: 2
report_to: ["none"]
